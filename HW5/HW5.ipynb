{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW5.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "3DPxOsT7CBc9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Welcome to Homework 5 (CS 498 AML)!"
      ]
    },
    {
      "metadata": {
        "id": "CkTu4megE9is",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy  as np\n",
        "import random\n",
        "from math import ceil\n",
        "from math import floor\n",
        "from os import listdir\n",
        "from sklearn.cluster  import KMeans\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics  import confusion_matrix, accuracy_score\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GP4pcyBNKlD3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# The Dataset is composed of the recordings of 14 simple ADL\n",
        "ADL_Names = [\"Brush_teeth\", \"Climb_stairs\", \"Comb_hair\", \"Descend_stairs\", \"Drink_glass\", \n",
        "               \"Eat_meat\", \"Eat_soup\", \"Getup_bed\", \"Liedown_bed\", \"Pour_water\", \n",
        "               \"Sitdown_chair\", \"Standup_chair\", \"Use_telephone\", \"Walk\"]\n",
        "\n",
        "# ALL INPUT in this cell\n",
        "# dataIN[ADL][NUM_Matrix] stores one feature item (a nrow*3 array) for a certain ADL\n",
        "# dataIN is split randomly into train and test (4:1)\n",
        "# test is held out to evaluate accuracy\n",
        "dataIN = []\n",
        "train = []\n",
        "test = []\n",
        "for i,ADL in enumerate(ADL_Names):\n",
        "    # create a new list for an ADL\n",
        "    dataIN.append([])\n",
        "    # append all the files of this ADL in the list\n",
        "    prefix = 'HMP_Dataset/' + ADL + '/'\n",
        "    for filename in listdir(prefix):\n",
        "        if (filename == \".DS_Store\"):\n",
        "            continue\n",
        "        dataIN[i].append(pd.read_table(prefix + filename, sep = ' ', header = None).values)\n",
        "    # test-train split (80% for train, 20% for test)\n",
        "    random.shuffle(dataIN[i])\n",
        "    test.append(dataIN[i][0:ceil(len(dataIN[i])/5)])\n",
        "    train.append(dataIN[i][ceil(len(dataIN[i])/5):])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wxRwQJ3-q18E",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# three important parameters for question (b)\n",
        "FIXED_LEN  = 32;    # the size of the fixed length samples that you use (textbook value: 32)\n",
        "NCLUSTER_1 = 40;    # the number of cluster centers at the first level in the hierarchical k-means (textbook value: 40)\n",
        "NCLUSTER_2 = 12;    # the number of cluster centers at the second level in the hierarchical k-means (textbook value: 12)\n",
        "NCLUSTER   = 480;   # the number of cluster centers for normal k-means"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8qs9IQ6zNGM_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#####################################################################\n",
        "#\n",
        "# slice: function used to slice data into several non-overlapping\n",
        "#        equal length pieces\n",
        "# input:\n",
        "#       @data: the data to be sliced\n",
        "#       @fixed_len: length of each sliced piece\n",
        "# output:\n",
        "#       @segments: a 2-demension list that contains each file in its\n",
        "#                  first dimension, and sliced pieces for each file \n",
        "#                  in the corresponding second dimension\n",
        "#\n",
        "#####################################################################\n",
        "def slice(data, fixed_len, overlap_percent):\n",
        "  step = floor(fixed_len * (1 -overlap_percent))\n",
        "  segments = []\n",
        "  # for each folder\n",
        "  for i in range(len(data)):\n",
        "    # for each file\n",
        "    for j in range(len(data[i])):\n",
        "      # perform slicing\n",
        "      for x in range(fixed_len, len(data[i][j]), step):\n",
        "        tmp = []\n",
        "        # convert the matrix into one dimension vector\n",
        "        # by joining each row\n",
        "        for n in range(fixed_len):\n",
        "          tmp.extend(data[i][j][x-fixed_len:x][n])\n",
        "        segments.append(tmp)\n",
        "  # print(\"Obtain \", len(segments), \" pieces from slicing\")\n",
        "  return segments"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1M10GVN4hzn_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#####################################################################\n",
        "#\n",
        "# get_label: function used to get the label of data\n",
        "# input:\n",
        "#       @data: the input data, whose structure is:\n",
        "#              data[ADL][FileNumber]\n",
        "#       @LabelNames: a list contain all the label names\n",
        "# output:\n",
        "#       @label: a 1-dimension list that contains the corresponding\n",
        "#               label for the data\n",
        "#\n",
        "#####################################################################\n",
        "def get_label(data, LabelNames):\n",
        "  label = []\n",
        "  # for each folder\n",
        "  for i in range(len(data)):\n",
        "    # for each file\n",
        "    for j in range(len(data[i])):\n",
        "      label.append(LabelNames[i])\n",
        "  return label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w863Qy94_6lo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#####################################################################\n",
        "#\n",
        "# _kmeans_classify: helper function called by make_histograms,\n",
        "#                   used to classify each pieces of data into\n",
        "#                   given clusters\n",
        "# input:\n",
        "#       @data: the input data, whose structure is:\n",
        "#              data[ADL][FileNumber]\n",
        "#       @fixed_len: length of each sliced piece\n",
        "#       @kmeans_model: kmeans model used to apply cluster\n",
        "# output:\n",
        "#       @classification: a 2-demension list that contains each \n",
        "#                        file in its first dimension, \n",
        "#                        and sliced pieces classification for \n",
        "#                        each file in the corresponding \n",
        "#                        second dimension\n",
        "#\n",
        "##################################################################### \n",
        "def _kmeans_classify(data, fixed_len, kmeans_model):\n",
        "  classification = []\n",
        "  file_count = 0\n",
        "  # for each folder\n",
        "  for i in range(len(data)):\n",
        "    # for each file\n",
        "    for j in range(len(data[i])):\n",
        "      file_count += 1\n",
        "      file_classification = []\n",
        "      # for each sliced piece\n",
        "      for x in range(fixed_len, len(data[i][j]),fixed_len):\n",
        "        # due to the constraint, sklearn kmeans,\n",
        "        # we have to make sliced_piece into 2 dimensions\n",
        "        sliced_piece = []\n",
        "        tmp = []\n",
        "        for n in range(fixed_len):\n",
        "          tmp.extend(data[i][j][x-fixed_len:x][n])\n",
        "        sliced_piece.append(tmp)\n",
        "        # classify\n",
        "        file_classification.extend(kmeans_model.predict(np.array(sliced_piece)))\n",
        "      classification.append(file_classification)\n",
        "  # print(\"Total number of test files is:\", file_count)\n",
        "  # print(\"Total number of prediction is:\", len(classification))\n",
        "  return classification"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pwTDPAI1EG_G",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#####################################################################\n",
        "#\n",
        "# normalize: function used to normalize the given data set\n",
        "# input:\n",
        "#       @data: the input data, whose structure is:\n",
        "#              data[file][feature]\n",
        "# output:\n",
        "#       @list_normalized: a 2-demension list that contains each \n",
        "#                         file in its first dimension, \n",
        "#                         and corresponding normalized features \n",
        "#                         in its second dimension\n",
        "#\n",
        "#####################################################################\n",
        "def normalize(data):\n",
        "    # for each file\n",
        "    for row in range(len(data)):\n",
        "        sum = 0\n",
        "        # calculate the sum\n",
        "        for col in range(len(data[row])):\n",
        "            sum += data[row][col]\n",
        "        # diviede each feature by the sum\n",
        "        data[row] = list(np.array(data[row]) / sum)\n",
        "    return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xRaugz6Vc30P",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#####################################################################\n",
        "#\n",
        "# make_histograms: function used to convert given data into \n",
        "#                   histograms (namely, extract equal length feature\n",
        "#                   out of given data)\n",
        "# input:\n",
        "#       @data: the input data, whose structure is:\n",
        "#              data[ADL][FileNumber]\n",
        "#       @fixed_len: length of each sliced piece\n",
        "#       @kmeans_model: kmeans model used to apply cluster\n",
        "#       @clusterNum: total number of clusters\n",
        "# output:\n",
        "#       @histograms: a 2-demension list that contains each \n",
        "#                    file in its first dimension, \n",
        "#                    and corresponding features in its \n",
        "#                    second dimension \n",
        "#\n",
        "##################################################################### \n",
        "def make_histograms(data, fixed_len, kmeans_model, clusterNum):\n",
        "  # call _kmeans_classify to slice and classify\n",
        "  prediction = _kmeans_classify(data, fixed_len, kmeans_model)\n",
        "  histograms = []\n",
        "  # for each file\n",
        "  for fileNum in range(len(prediction)):\n",
        "    tmp_histograms = [0] * clusterNum\n",
        "    # count the number of each cluster and build histograms based on that\n",
        "    for cluster in prediction[fileNum]:\n",
        "      tmp_histograms[cluster] += 1\n",
        "    histograms.append(tmp_histograms)\n",
        "  return histograms"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_3HgDzx7zDhC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def plot_activity_histogram(all_data, all_label):\n",
        "    # dict used to record plotted times\n",
        "    plotted =dict()\n",
        "    # for each file\n",
        "    for row in range(len(all_data)):\n",
        "        # plot each activity twice\n",
        "        if all_label[row] in plotted.keys():\n",
        "            if plotted[all_label[row]] < 2:\n",
        "                plotted[all_label[row]] += 1\n",
        "                plot_activity_histogram_helper(all_data[row], all_label[row], plotted[all_label[row]])\n",
        "            else:\n",
        "                continue\n",
        "        else:\n",
        "            plotted[all_label[row]] = 1\n",
        "            plot_activity_histogram_helper(all_data[row], all_label[row], plotted[all_label[row]])\n",
        "    return\n",
        "\n",
        "def plot_activity_histogram_helper(data, label, times):\n",
        "    fig = plt.figure()\n",
        "    x = range(len(data))\n",
        "    plt.title(label)\n",
        "    plt.bar(x, data, width=3)\n",
        "    filename = 'plot/'+ label + '_' + str(times) + '.png'\n",
        "    plt.savefig(filename)\n",
        "    plt.close('all')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZFZYT2lDshZB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#####################################################################\n",
        "#\n",
        "# classifier: function to classify based on vector quantization and k-means\n",
        "# input:\n",
        "#       @fixed_len: length of each sliced piece\n",
        "#       @ncluster: total number of clusters\n",
        "# output:\n",
        "#       @confusionMatrix: the confusion matrix of the classifier\n",
        "#       @accuracy: the accuracy on the held out test dataset\n",
        "#\n",
        "#####################################################################\n",
        "def classifier(fixed_len, ncluster, overlap_percent, plot = False):\n",
        "    # VECTOR QUANTIZE & PREPARE FEATURES + LABELS\n",
        "    # break signals into sample segments\n",
        "    segments = slice(train, fixed_len, overlap_percent)\n",
        "    # normal k-means (480 cluster centers)\n",
        "    kmeans = KMeans(n_clusters = ncluster, random_state = 0).fit(np.array(segments))\n",
        "    # making features using histogram of cluster centers\n",
        "    trainFeatures = make_histograms(train, fixed_len, kmeans, ncluster)\n",
        "    testFeatures = make_histograms(test, fixed_len, kmeans, ncluster)\n",
        "    # normalize the histograms to get rid of the influence caused by the length of files\n",
        "    trainFeatures_normalized = normalize(trainFeatures)\n",
        "    testFeatures_normalized = normalize(testFeatures)\n",
        "    # get the ground truth labels for both train & test dataset\n",
        "    trainLabel = get_label(train, ADL_Names)\n",
        "    testLabel = get_label(test, ADL_Names)\n",
        "    \n",
        "    if (plot == True):\n",
        "      plot_activity_histogram(trainFeatures_normalized, trainLabel)\n",
        "\n",
        "    # CLASSIFICATION\n",
        "    # Random Forest Prediction\n",
        "    RandomForestModel = RandomForestClassifier(n_estimators = 100)\n",
        "    RandomForestModel.fit(trainFeatures_normalized, trainLabel)\n",
        "    test_pred = RandomForestModel.predict(testFeatures_normalized)\n",
        "\n",
        "    # confusion matrix\n",
        "    confusionMatrix = confusion_matrix(test_pred, testLabel)\n",
        "    # accuracy of prediction\n",
        "    accuracy = accuracy_score(test_pred, testLabel)\n",
        "    \n",
        "    return (confusionMatrix, accuracy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "i7BTYigea6vH",
        "colab_type": "code",
        "outputId": "af7e15ec-8492-4990-d946-7b296c880f01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "cell_type": "code",
      "source": [
        "FIXED_LEN = 10\n",
        "NCLUSTER = 226\n",
        "OVERLAP_PERCENT = 0.1\n",
        "# evaluate the classifier & get confusion matrix + accuracy\n",
        "(confusionMatrix, accuracy) = classifier(FIXED_LEN, NCLUSTER, OVERLAP_PERCENT, plot = False)\n",
        "# report (a) the total error rate and (b) the class confusion matrix of the classifier\n",
        "print(\"The total error rate is: \", 1 - accuracy)\n",
        "print(\"The class confusion matrix is: \\n\", confusionMatrix)\n",
        "dataframe = pd.DataFrame(confusionMatrix)\n",
        "dataframe.to_csv(\"confusionMatrix.csv\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Obtain  39242  pieces from slicing\n",
            "Total number of test files is: 666\n",
            "Total number of prediction is: 666\n",
            "Total number of test files is: 173\n",
            "Total number of prediction is: 173\n",
            "The total error rate is:  0.21387283236994215\n",
            "The class confusion matrix is: \n",
            " [[ 3  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0 17  0  0  0  0  0  0  0  0  0  0  0  2]\n",
            " [ 0  0  7  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  8  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0 20  0  0  0  0  0  0  0  2  0]\n",
            " [ 0  0  0  0  0  1  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  1  0  0  0  0  0  0  0]\n",
            " [ 0  1  0  0  0  0  0 17  4  0  0  1  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0 20  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  1  2  0 14  7  0  3]\n",
            " [ 0  0  0  0  0  0  0  3  0  0  6 13  0  1]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  1  0]\n",
            " [ 0  3  0  1  0  0  0  0  0  0  0  0  0 14]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "COI0eLm90wtZ",
        "colab_type": "code",
        "outputId": "a46cecbf-f7b7-4b9c-d991-3feef30c7970",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1004
        }
      },
      "cell_type": "code",
      "source": [
        "# Modify the number of cluster centers & size of the fixed length to optimize\n",
        "max_accuracy = 0\n",
        "best_fixed_len = 0\n",
        "best_ncluster = 0\n",
        "bset_overlap_percent = 0\n",
        "\n",
        "# cache\n",
        "input_cache = []\n",
        "output_cache = []\n",
        "\n",
        "total = ((100 - 10) / 10) * ((1000 - 200)/100) * ((0.6 - 0) / 0.1)\n",
        "curr = 0\n",
        "\n",
        "for fixed_len in range(10, 100, 10):\n",
        "    for ncluster in range(200, 1000, 100):\n",
        "        for overlap_percent in np.arange(0.0, 0.6, 0.1):\n",
        "            curr += 1\n",
        "            print(\"Processing ... \", (curr / total)*100, \"%\")\n",
        "            \n",
        "            curr_input = [fixed_len, ncluster, overlap_percent]\n",
        "            input_cache.append(curr_input)\n",
        "\n",
        "            count = 0\n",
        "            for times in range(3):\n",
        "                count += classifier(fixed_len, ncluster, overlap_percent, plot = False)[1]\n",
        "            accuracy = count/3\n",
        "\n",
        "            output_cache.append(accuracy)\n",
        "\n",
        "            if (accuracy > max_accuracy):\n",
        "                max_accuracy = accuracy\n",
        "                best_fixed_len = fixed_len\n",
        "                best_ncluster = ncluster\n",
        "                bset_overlap_percent = overlap_percent\n",
        "\n",
        "print(max_accuracy, \" \", best_fixed_len, \" \", best_ncluster, \" \", bset_overlap_percent)\n",
        "cache = {'input': input_cache, 'accuracy': output_cache}\n",
        "df_cache = pd.DataFrame(data=cache)\n",
        "df_cache.to_csv(\"cache.csv\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing ...  0.2314814814814815 %\n",
            "Processing ...  0.462962962962963 %\n",
            "Processing ...  0.6944444444444445 %\n",
            "Processing ...  0.925925925925926 %\n",
            "Processing ...  1.1574074074074074 %\n",
            "Processing ...  1.388888888888889 %\n",
            "Processing ...  1.6203703703703707 %\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-75db385ac30e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mtimes\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m                 \u001b[0mcount\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfixed_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mncluster\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverlap_percent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-329eb525c0ac>\u001b[0m in \u001b[0;36mclassifier\u001b[0;34m(fixed_len, ncluster, overlap_percent, plot)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0msegments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfixed_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverlap_percent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# normal k-means (480 cluster centers)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mkmeans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_clusters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mncluster\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msegments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;31m# making features using histogram of cluster centers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mtrainFeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_histograms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfixed_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkmeans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mncluster\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/cluster/k_means_.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy_x\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m                 \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malgorithm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malgorithm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 896\u001b[0;31m                 return_n_iter=True)\n\u001b[0m\u001b[1;32m    897\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/cluster/k_means_.py\u001b[0m in \u001b[0;36mk_means\u001b[0;34m(X, n_clusters, init, precompute_distances, n_init, max_iter, verbose, tol, random_state, copy_x, n_jobs, algorithm, return_n_iter)\u001b[0m\n\u001b[1;32m    344\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_clusters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m                 \u001b[0mprecompute_distances\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprecompute_distances\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m                 x_squared_norms=x_squared_norms, random_state=random_state)\n\u001b[0m\u001b[1;32m    347\u001b[0m             \u001b[0;31m# determine if these results are the best so far\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbest_inertia\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0minertia\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mbest_inertia\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/cluster/k_means_.py\u001b[0m in \u001b[0;36m_kmeans_single_elkan\u001b[0;34m(X, n_clusters, max_iter, init, verbose, x_squared_norms, random_state, tol, precompute_distances)\u001b[0m\n\u001b[1;32m    398\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Initialization complete'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m     centers, labels, n_iter = k_means_elkan(X, n_clusters, centers, tol=tol,\n\u001b[0;32m--> 400\u001b[0;31m                                             max_iter=max_iter, verbose=verbose)\n\u001b[0m\u001b[1;32m    401\u001b[0m     \u001b[0minertia\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mcenters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minertia\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcenters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32msklearn/cluster/_k_means_elkan.pyx\u001b[0m in \u001b[0;36msklearn.cluster._k_means_elkan.k_means_elkan\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36meuclidean_distances\u001b[0;34m(X, Y, Y_norm_squared, squared, X_norm_squared)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;31m# Pairwise distances\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m def euclidean_distances(X, Y=None, Y_norm_squared=None, squared=False,\n\u001b[0m\u001b[1;32m    164\u001b[0m                         X_norm_squared=None):\n\u001b[1;32m    165\u001b[0m     \"\"\"\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "VVwGSM3pibUp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "LAB TEST AREA BELOW"
      ]
    },
    {
      "metadata": {
        "id": "ywjk2vpeRJSG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iOpZa2JfWQKm",
        "colab_type": "code",
        "outputId": "31d5399b-2f32-40ce-c0d9-6d1e097eb892",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        }
      },
      "cell_type": "code",
      "source": [
        "print(FIXED_LEN, NCLUSTER)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-f7c517035d2b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFIXED_LEN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNCLUSTER\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'FIXED_LEN' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "fxvTeOd9iz04",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "kmeans = KMeans(n_clusters = NCLUSTER_1, random_state = 0).fit(np.array(segments))\n",
        "print(kmeans.cluster_centers)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QV8AUCEpUufx",
        "colab_type": "code",
        "outputId": "7256af99-d2ef-4f85-c671-b3a19238562f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "np.array([[0,2],[1,4]])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 2],\n",
              "       [1, 4]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "metadata": {
        "id": "WKxec07cJb0E",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        " Upload the data zip and unzip it (do it every time)"
      ]
    },
    {
      "metadata": {
        "id": "EA0Gf1sfFvvl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# unzip\n",
        "import zipfile\n",
        "Zipfile = zipfile.ZipFile('ADL_Dataset.zip', 'r')\n",
        "Zipfile.extractall()\n",
        "# remove the folder\n",
        "# import shutil\n",
        "# shutil.rmtree('ADL_Dataset')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xDFqcXDHrYTR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# DRAFT\n",
        "# vector quantize\n",
        "# three important parameters for question (b)\n",
        "FIXED_LEN = 32;     # the size of the fixed length samples that you use (textbook value: 32)\n",
        "NCLUSTER_1 = 40;    # the number of cluster centers at the first level in the hierarchical k-means (textbook value: 40)\n",
        "NCLUSTER_2 = 12;    # the number of cluster centers at the second level in the hierarchical k-means (textbook value: 12)\n",
        "\n",
        "segments = []\n",
        "for i in range(len(train)):\n",
        "    for j in range(len(train[i])):\n",
        "        # for a file (k*3 matrix), cut into a bunch of FIXED_LEN*3 matrix\n",
        "        segments.extend([train[i][j][x-FIXED_LEN:x][:] for x in range(FIXED_LEN, len(train[i][j]),FIXED_LEN)])\n",
        "#     print(len(segments))\n",
        "# print(len(segments)) # around 11000 segments in total\n",
        "\n",
        "\n",
        "# Hierarchical k-means (two-level)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1HujYMNPx-L6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# convert (m x n) matrix to (1 x (m*n)) vector \n",
        "# vector quantize\n",
        "segments = []\n",
        "for i in range(len(train)):\n",
        "    for j in range(len(train[i])):\n",
        "        # for a file (k*3 matrix), cut into a bunch of FIXED_LEN*3 matrix\n",
        "        for x in range(FIXED_LEN, len(train[i][j]),FIXED_LEN):\n",
        "            tmp = []\n",
        "            for n in range(FIXED_LEN):\n",
        "                tmp.extend(train[i][j][x-FIXED_LEN:x][n])\n",
        "            segments.append(tmp)\n",
        "# print the number of segments\n",
        "print(\"The number of segments is: \", len(segments))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}